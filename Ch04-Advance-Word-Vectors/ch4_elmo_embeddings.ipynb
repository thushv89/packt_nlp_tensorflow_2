{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3e0e19",
   "metadata": {},
   "source": [
    "## Using ELMo Embeddings\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/packt_nlp_tensorflow_2/blob/master/Ch04-Advance-Word-Vectors/ch4_elmo_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6647d3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n",
      "env: TFHUB_CACHE_DIR=./tfhub_modules\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "# Making sure we cache the models and are not downloaded all the time\n",
    "%env TFHUB_CACHE_DIR=./tfhub_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08272665",
   "metadata": {},
   "source": [
    "## Using pre-trained ELMo Model\n",
    "\n",
    "### Downloading the ELMo Model from TFHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c5a1d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Remove any ongoing sessions\n",
    "K.clear_session()\n",
    "\n",
    "# Download the ELMo model and save to disk\n",
    "elmo_layer = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\", signature=\"tokens\",signature_outputs_as_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e46bcf",
   "metadata": {},
   "source": [
    "### Formatting the input for ELMo\n",
    "\n",
    "ELMo expects the inputs to be in a specific format. Here we write a function to get the input in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f0cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': <tf.Tensor: shape=(2, 6), dtype=string, numpy=\n",
      "array([[b'the', b'cat', b'sat', b'on', b'the', b'mat'],\n",
      "       [b'the', b'mat', b'sat', b'', b'', b'']], dtype=object)>, 'sequence_len': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 3])>}\n"
     ]
    }
   ],
   "source": [
    "def format_text_for_elmo(texts, lower=True, split=\" \", max_len=None):\n",
    "    \n",
    "    \"\"\" Formats a given text for the ELMo model (takes in a list of strings) \"\"\"\n",
    "        \n",
    "    token_inputs = [] # Maintains individual tokens\n",
    "    token_lengths = [] # Maintains the length of each sequence\n",
    "    \n",
    "    max_len_inferred = 0 # We keep a variable to matain the max length of the input\n",
    "    \n",
    "    # Go through each text (string)\n",
    "    for text in texts:    \n",
    "        \n",
    "        # Process the text and get a list of tokens\n",
    "        tokens = tf.keras.preprocessing.text.text_to_word_sequence(text, lower=lower, split=split)\n",
    "        \n",
    "        # Add the tokens \n",
    "        token_inputs.append(tokens)                   \n",
    "        \n",
    "        # Compute the max length for the collection of sequences\n",
    "        if len(tokens)>max_len_inferred:\n",
    "            max_len_inferred = len(tokens)\n",
    "    \n",
    "    # It's important to make sure the maximum token length is only as large as the longest input in the sequence\n",
    "    # You can't have arbitrarily large length as the maximum length. Otherwise, you'll get this error.\n",
    "    #InvalidArgumentError:  Incompatible shapes: [2,6,1] vs. [2,10,1024]\n",
    "    #    [[node mul (defined at .../python3.6/site-packages/tensorflow_hub/module_v2.py:106) ]] [Op:__inference_pruned_3391]\n",
    "    \n",
    "    # Here we make sure max_len is only as large as the longest input\n",
    "    if max_len and max_len_inferred < max_len:\n",
    "        max_len = max_len_inferred\n",
    "    if not max_len:\n",
    "        max_len = max_len_inferred\n",
    "    \n",
    "    # Go through each token sequence and modify sequences to have same length\n",
    "    for i, token_seq in enumerate(token_inputs):\n",
    "        \n",
    "        token_lengths.append(min(len(token_seq), max_len))\n",
    "        \n",
    "        # If the maximum length is less than input length, truncate\n",
    "        if max_len < len(token_seq):\n",
    "            token_seq = token_seq[:max_len]            \n",
    "        # If the maximum length is greater than or equal to input length, add padding as needed\n",
    "        else:            \n",
    "            token_seq = token_seq+[\"\"]*(max_len-len(token_seq))\n",
    "                \n",
    "        assert len(token_seq)==max_len\n",
    "        \n",
    "        token_inputs[i] = token_seq\n",
    "    \n",
    "    # Return the final output\n",
    "    return {\n",
    "        \"tokens\": tf.constant(token_inputs), \n",
    "        \"sequence_len\": tf.constant(token_lengths)\n",
    "    }\n",
    "\n",
    "\n",
    "print(format_text_for_elmo([\"the cat sat on the mat\", \"the mat sat\"], max_len=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45986d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor under key=word_emb is a (5, 6, 512) shaped Tensor\n",
      "Tensor under key=lstm_outputs2 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=sequence_len is a (5,) shaped Tensor\n",
      "Tensor under key=elmo is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=lstm_outputs1 is a (5, 6, 1024) shaped Tensor\n",
      "Tensor under key=default is a (5, 1024) shaped Tensor\n"
     ]
    }
   ],
   "source": [
    "# Titles of 001.txt - 005.txt in bbc/business\n",
    "elmo_inputs = format_text_for_elmo([\n",
    "    \"Ad sales boost Time Warner profit\",\n",
    "    \"Dollar gains on Greenspan speech\",\n",
    "    \"Yukos unit buyer faces loan claim\",\n",
    "    \"High fuel prices hit BA's profits\",\n",
    "    \"Pernod takeover talk lifts Domecq\"\n",
    "])\n",
    "\n",
    "# Get the result from ELMo\n",
    "elmo_result = elmo_layer(elmo_inputs)\n",
    "\n",
    "# Print the result\n",
    "for k,v in elmo_result.items():    \n",
    "    print(f\"Tensor under key={k} is a {v.shape} shaped Tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb329b",
   "metadata": {},
   "source": [
    "## Generating Document Embeddings with ELMo\n",
    "\n",
    "### Downloading the data\n",
    "\n",
    "This code downloads a [BBC dataset](hhttp://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) consisting of news articles published by BBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801a2bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "bbc-fulltext.zip has already been extracted\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
    "\n",
    "\n",
    "def download_data(url, data_dir):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    \n",
    "    # Create the data directory if not exist\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
    "    \n",
    "    # If file doesnt exist, download\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Downloading file...')\n",
    "        filename, _ = urlretrieve(url, file_path)\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "  \n",
    "    extract_path = os.path.join(data_dir, 'bbc')\n",
    "    \n",
    "    # If data has not been extracted already, extract data\n",
    "    if not os.path.exists(extract_path):        \n",
    "        with zipfile.ZipFile(os.path.join(data_dir, 'bbc-fulltext.zip'), 'r') as zipf:\n",
    "            zipf.extractall(data_dir)\n",
    "    else:\n",
    "        print(\"bbc-fulltext.zip has already been extracted\")\n",
    "    \n",
    "download_data(url, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416dba6e",
   "metadata": {},
   "source": [
    "### Read Data without Preprocessing \n",
    "\n",
    "Here we read all the files and keep them as a list of strings, where each string is a single article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b0a5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      ".................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 401.txt\n",
      "Detected 2226 stories\n",
      "865250 words found in the total news set\n",
      "Example words (start):  Consists of 2225 documents from the BBC news websi\n",
      "Example words (end):  Online was the game, ahhhh them was the days ! LOL\n"
     ]
    }
   ],
   "source": [
    "def read_data(data_dir):\n",
    "    \n",
    "    # This will contain the full list of stories\n",
    "    news_stories = []    \n",
    "    filenames = []\n",
    "    print(\"Reading files\")\n",
    "    \n",
    "    i = 0 # Just used for printing progress\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        \n",
    "        for fi, f in enumerate(files):\n",
    "            \n",
    "            # We don't read the readme file\n",
    "            if 'README' in f:\n",
    "                continue\n",
    "            \n",
    "            # Printing progress\n",
    "            i += 1\n",
    "            print(\".\"*i, f, end='\\r')\n",
    "            \n",
    "            # Open the file\n",
    "            with open(os.path.join(root, f), encoding='latin-1') as text_file:\n",
    "                \n",
    "                story = []\n",
    "                # Read all the lines\n",
    "                for row in text_file:\n",
    "                                        \n",
    "                    story.append(row.strip())\n",
    "                    \n",
    "                # Create a single string with all the rows in the doc\n",
    "                story = ' '.join(story)                        \n",
    "                # Add that to the list\n",
    "                news_stories.append(story)  \n",
    "                filenames.append(os.path.join(root, f))\n",
    "                \n",
    "        print('', end='\\r')\n",
    "        \n",
    "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
    "    return news_stories, filenames\n",
    "                \n",
    "  \n",
    "news_stories, filenames = read_data(os.path.join('data', 'bbc'))\n",
    "\n",
    "# Printing some stats and sample data\n",
    "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
    "print('Example words (start): ',news_stories[0][:50])\n",
    "print('Example words (end): ',news_stories[-1][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ff4ec",
   "metadata": {},
   "source": [
    "### Check the length statistics \n",
    "\n",
    "Here we look at the 95-percientile in order to decide a good sequence length for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3337475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2226.000000\n",
       "mean      388.701707\n",
       "std       241.514748\n",
       "min        87.000000\n",
       "5%        164.000000\n",
       "50%       336.000000\n",
       "95%       736.750000\n",
       "max      4489.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series([len(x.split(' ')) for x in news_stories]).describe(percentiles=[0.05, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a8307",
   "metadata": {},
   "source": [
    "### Compute the document embeddings\n",
    "\n",
    "ELMo provides several outputs as the output (in the form of a dictionary). The most important output is in a key called `default` which is the averaged vector resulting from vectors produced for all the tokens in the input. We will use this as the document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d915657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "news_elmo_embeddings = []\n",
    "\n",
    "# Go through batches\n",
    "for i in range(0, len(news_stories), batch_size):\n",
    "    \n",
    "    # Print progress\n",
    "    print('.', end='')\n",
    "    # Format ELMo inputs\n",
    "    elmo_inputs = format_text_for_elmo(news_stories[i: min(i+batch_size, len(news_stories))], max_len=768)    \n",
    "    # Get the result stored in default\n",
    "    elmo_result = elmo_layer(elmo_inputs)[\"default\"]\n",
    "    # Add that to a list\n",
    "    news_elmo_embeddings.append(elmo_result)\n",
    "\n",
    "# Create an array\n",
    "news_elmo_embeddings = np.concatenate(news_elmo_embeddings, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb919f",
   "metadata": {},
   "source": [
    "### Save the embeddings to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf38bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to disk\n",
    "os.makedirs('elmo_embeddings', exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    news_elmo_embeddings, index=filenames\n",
    ").to_pickle(\n",
    "    os.path.join('elmo_embeddings', 'elmo_embeddings.pkl')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4aaa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\readme.txt</th>\n",
       "      <td>0.317657</td>\n",
       "      <td>-0.037904</td>\n",
       "      <td>0.096557</td>\n",
       "      <td>-0.120920</td>\n",
       "      <td>-0.031368</td>\n",
       "      <td>0.100202</td>\n",
       "      <td>-0.026247</td>\n",
       "      <td>0.396833</td>\n",
       "      <td>-0.147786</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287333</td>\n",
       "      <td>0.232494</td>\n",
       "      <td>0.078371</td>\n",
       "      <td>0.287477</td>\n",
       "      <td>0.104193</td>\n",
       "      <td>0.087736</td>\n",
       "      <td>0.313276</td>\n",
       "      <td>-0.105956</td>\n",
       "      <td>0.213654</td>\n",
       "      <td>0.086695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\business\\001.txt</th>\n",
       "      <td>0.052209</td>\n",
       "      <td>-0.108837</td>\n",
       "      <td>0.103078</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.277382</td>\n",
       "      <td>0.101622</td>\n",
       "      <td>0.147006</td>\n",
       "      <td>0.390242</td>\n",
       "      <td>0.053942</td>\n",
       "      <td>-0.051409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179657</td>\n",
       "      <td>0.053069</td>\n",
       "      <td>0.091880</td>\n",
       "      <td>0.257571</td>\n",
       "      <td>0.044403</td>\n",
       "      <td>-0.093912</td>\n",
       "      <td>0.032205</td>\n",
       "      <td>-0.116896</td>\n",
       "      <td>0.420686</td>\n",
       "      <td>0.049002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\business\\002.txt</th>\n",
       "      <td>-0.277251</td>\n",
       "      <td>-0.498861</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>0.090846</td>\n",
       "      <td>0.320750</td>\n",
       "      <td>-0.054692</td>\n",
       "      <td>-0.041421</td>\n",
       "      <td>0.329660</td>\n",
       "      <td>-0.438261</td>\n",
       "      <td>-0.232513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140432</td>\n",
       "      <td>-0.022603</td>\n",
       "      <td>0.369779</td>\n",
       "      <td>0.214081</td>\n",
       "      <td>-0.019910</td>\n",
       "      <td>-0.004382</td>\n",
       "      <td>-0.073545</td>\n",
       "      <td>0.050382</td>\n",
       "      <td>0.697808</td>\n",
       "      <td>-0.038186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\business\\003.txt</th>\n",
       "      <td>0.001640</td>\n",
       "      <td>-0.078992</td>\n",
       "      <td>0.178254</td>\n",
       "      <td>-0.076050</td>\n",
       "      <td>-0.188926</td>\n",
       "      <td>0.156753</td>\n",
       "      <td>-0.178013</td>\n",
       "      <td>0.316145</td>\n",
       "      <td>0.339274</td>\n",
       "      <td>0.103716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077891</td>\n",
       "      <td>0.137827</td>\n",
       "      <td>0.199895</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>-0.148592</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>-0.008611</td>\n",
       "      <td>0.582661</td>\n",
       "      <td>-0.055719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\business\\004.txt</th>\n",
       "      <td>-0.176339</td>\n",
       "      <td>-0.215292</td>\n",
       "      <td>0.215862</td>\n",
       "      <td>0.094050</td>\n",
       "      <td>0.498871</td>\n",
       "      <td>0.291168</td>\n",
       "      <td>-0.037569</td>\n",
       "      <td>0.238697</td>\n",
       "      <td>0.275630</td>\n",
       "      <td>0.069215</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472698</td>\n",
       "      <td>0.062436</td>\n",
       "      <td>0.207713</td>\n",
       "      <td>0.127696</td>\n",
       "      <td>0.203626</td>\n",
       "      <td>-0.116648</td>\n",
       "      <td>0.153931</td>\n",
       "      <td>-0.271529</td>\n",
       "      <td>0.392614</td>\n",
       "      <td>0.046904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\tech\\397.txt</th>\n",
       "      <td>0.211539</td>\n",
       "      <td>0.073864</td>\n",
       "      <td>-0.228550</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>0.234441</td>\n",
       "      <td>0.221942</td>\n",
       "      <td>-0.166678</td>\n",
       "      <td>0.108783</td>\n",
       "      <td>-0.160235</td>\n",
       "      <td>-0.005722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432023</td>\n",
       "      <td>-0.045294</td>\n",
       "      <td>0.049274</td>\n",
       "      <td>0.030276</td>\n",
       "      <td>0.052349</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>0.367616</td>\n",
       "      <td>-0.109777</td>\n",
       "      <td>0.069831</td>\n",
       "      <td>-0.080369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\tech\\398.txt</th>\n",
       "      <td>0.165850</td>\n",
       "      <td>0.008826</td>\n",
       "      <td>-0.154758</td>\n",
       "      <td>0.104231</td>\n",
       "      <td>0.289332</td>\n",
       "      <td>0.079604</td>\n",
       "      <td>-0.154873</td>\n",
       "      <td>-0.315355</td>\n",
       "      <td>-0.238488</td>\n",
       "      <td>0.168557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419998</td>\n",
       "      <td>0.051438</td>\n",
       "      <td>-0.045387</td>\n",
       "      <td>0.279227</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>0.104169</td>\n",
       "      <td>0.212102</td>\n",
       "      <td>-0.126483</td>\n",
       "      <td>0.122837</td>\n",
       "      <td>-0.329561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\tech\\399.txt</th>\n",
       "      <td>-0.066523</td>\n",
       "      <td>-0.190664</td>\n",
       "      <td>0.104273</td>\n",
       "      <td>-0.145981</td>\n",
       "      <td>0.181983</td>\n",
       "      <td>-0.011872</td>\n",
       "      <td>-0.086985</td>\n",
       "      <td>0.176678</td>\n",
       "      <td>-0.357082</td>\n",
       "      <td>-0.030099</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234072</td>\n",
       "      <td>0.192155</td>\n",
       "      <td>0.049397</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>-0.009068</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.389549</td>\n",
       "      <td>0.052150</td>\n",
       "      <td>0.347691</td>\n",
       "      <td>-0.027860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\tech\\400.txt</th>\n",
       "      <td>-0.054691</td>\n",
       "      <td>-0.251641</td>\n",
       "      <td>-0.107450</td>\n",
       "      <td>-0.015446</td>\n",
       "      <td>0.133443</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.091617</td>\n",
       "      <td>-0.058762</td>\n",
       "      <td>-0.347266</td>\n",
       "      <td>0.109252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390207</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.102706</td>\n",
       "      <td>0.475566</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>-0.224070</td>\n",
       "      <td>0.159726</td>\n",
       "      <td>-0.073267</td>\n",
       "      <td>0.323672</td>\n",
       "      <td>-0.324118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\bbc\\tech\\401.txt</th>\n",
       "      <td>0.175791</td>\n",
       "      <td>-0.200719</td>\n",
       "      <td>0.018892</td>\n",
       "      <td>-0.073387</td>\n",
       "      <td>0.167212</td>\n",
       "      <td>0.313639</td>\n",
       "      <td>0.091184</td>\n",
       "      <td>0.124715</td>\n",
       "      <td>-0.201694</td>\n",
       "      <td>0.044884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153042</td>\n",
       "      <td>0.278959</td>\n",
       "      <td>-0.054128</td>\n",
       "      <td>0.198412</td>\n",
       "      <td>0.025417</td>\n",
       "      <td>-0.032319</td>\n",
       "      <td>0.181318</td>\n",
       "      <td>-0.061342</td>\n",
       "      <td>0.503715</td>\n",
       "      <td>0.008395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2226 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0         1         2         3         4     \\\n",
       "data\\bbc\\readme.txt        0.317657 -0.037904  0.096557 -0.120920 -0.031368   \n",
       "data\\bbc\\business\\001.txt  0.052209 -0.108837  0.103078  0.060255  0.277382   \n",
       "data\\bbc\\business\\002.txt -0.277251 -0.498861 -0.000488  0.090846  0.320750   \n",
       "data\\bbc\\business\\003.txt  0.001640 -0.078992  0.178254 -0.076050 -0.188926   \n",
       "data\\bbc\\business\\004.txt -0.176339 -0.215292  0.215862  0.094050  0.498871   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "data\\bbc\\tech\\397.txt      0.211539  0.073864 -0.228550 -0.021582  0.234441   \n",
       "data\\bbc\\tech\\398.txt      0.165850  0.008826 -0.154758  0.104231  0.289332   \n",
       "data\\bbc\\tech\\399.txt     -0.066523 -0.190664  0.104273 -0.145981  0.181983   \n",
       "data\\bbc\\tech\\400.txt     -0.054691 -0.251641 -0.107450 -0.015446  0.133443   \n",
       "data\\bbc\\tech\\401.txt      0.175791 -0.200719  0.018892 -0.073387  0.167212   \n",
       "\n",
       "                               5         6         7         8         9     \\\n",
       "data\\bbc\\readme.txt        0.100202 -0.026247  0.396833 -0.147786 -0.080523   \n",
       "data\\bbc\\business\\001.txt  0.101622  0.147006  0.390242  0.053942 -0.051409   \n",
       "data\\bbc\\business\\002.txt -0.054692 -0.041421  0.329660 -0.438261 -0.232513   \n",
       "data\\bbc\\business\\003.txt  0.156753 -0.178013  0.316145  0.339274  0.103716   \n",
       "data\\bbc\\business\\004.txt  0.291168 -0.037569  0.238697  0.275630  0.069215   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "data\\bbc\\tech\\397.txt      0.221942 -0.166678  0.108783 -0.160235 -0.005722   \n",
       "data\\bbc\\tech\\398.txt      0.079604 -0.154873 -0.315355 -0.238488  0.168557   \n",
       "data\\bbc\\tech\\399.txt     -0.011872 -0.086985  0.176678 -0.357082 -0.030099   \n",
       "data\\bbc\\tech\\400.txt      0.000446 -0.091617 -0.058762 -0.347266  0.109252   \n",
       "data\\bbc\\tech\\401.txt      0.313639  0.091184  0.124715 -0.201694  0.044884   \n",
       "\n",
       "                           ...      1014      1015      1016      1017  \\\n",
       "data\\bbc\\readme.txt        ... -0.287333  0.232494  0.078371  0.287477   \n",
       "data\\bbc\\business\\001.txt  ... -0.179657  0.053069  0.091880  0.257571   \n",
       "data\\bbc\\business\\002.txt  ... -0.140432 -0.022603  0.369779  0.214081   \n",
       "data\\bbc\\business\\003.txt  ...  0.077891  0.137827  0.199895 -0.000208   \n",
       "data\\bbc\\business\\004.txt  ... -0.472698  0.062436  0.207713  0.127696   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "data\\bbc\\tech\\397.txt      ... -0.432023 -0.045294  0.049274  0.030276   \n",
       "data\\bbc\\tech\\398.txt      ... -0.419998  0.051438 -0.045387  0.279227   \n",
       "data\\bbc\\tech\\399.txt      ... -0.234072  0.192155  0.049397  0.103218   \n",
       "data\\bbc\\tech\\400.txt      ... -0.390207  0.009347  0.102706  0.475566   \n",
       "data\\bbc\\tech\\401.txt      ... -0.153042  0.278959 -0.054128  0.198412   \n",
       "\n",
       "                               1018      1019      1020      1021      1022  \\\n",
       "data\\bbc\\readme.txt        0.104193  0.087736  0.313276 -0.105956  0.213654   \n",
       "data\\bbc\\business\\001.txt  0.044403 -0.093912  0.032205 -0.116896  0.420686   \n",
       "data\\bbc\\business\\002.txt -0.019910 -0.004382 -0.073545  0.050382  0.697808   \n",
       "data\\bbc\\business\\003.txt  0.204819 -0.148592  0.030815 -0.008611  0.582661   \n",
       "data\\bbc\\business\\004.txt  0.203626 -0.116648  0.153931 -0.271529  0.392614   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "data\\bbc\\tech\\397.txt      0.052349  0.131190  0.367616 -0.109777  0.069831   \n",
       "data\\bbc\\tech\\398.txt      0.046775  0.104169  0.212102 -0.126483  0.122837   \n",
       "data\\bbc\\tech\\399.txt     -0.009068  0.135200  0.389549  0.052150  0.347691   \n",
       "data\\bbc\\tech\\400.txt      0.006273 -0.224070  0.159726 -0.073267  0.323672   \n",
       "data\\bbc\\tech\\401.txt      0.025417 -0.032319  0.181318 -0.061342  0.503715   \n",
       "\n",
       "                               1023  \n",
       "data\\bbc\\readme.txt        0.086695  \n",
       "data\\bbc\\business\\001.txt  0.049002  \n",
       "data\\bbc\\business\\002.txt -0.038186  \n",
       "data\\bbc\\business\\003.txt -0.055719  \n",
       "data\\bbc\\business\\004.txt  0.046904  \n",
       "...                             ...  \n",
       "data\\bbc\\tech\\397.txt     -0.080369  \n",
       "data\\bbc\\tech\\398.txt     -0.329561  \n",
       "data\\bbc\\tech\\399.txt     -0.027860  \n",
       "data\\bbc\\tech\\400.txt     -0.324118  \n",
       "data\\bbc\\tech\\401.txt      0.008395  \n",
       "\n",
       "[2226 rows x 1024 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(os.path.join('elmo_embeddings', 'elmo_embeddings.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867d719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
